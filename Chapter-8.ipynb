{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Chapter-8.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMx98amahU3vkZfvvzCZtLf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"sYsmWayLjOzp","colab_type":"text"},"source":["## **Трехслойная сеть для классификации набора данных MNIST**"]},{"cell_type":"code","metadata":{"id":"jWBLP01VhLM9","colab_type":"code","outputId":"e91dc4b1-ef18-4c6f-83c4-cb5d8d209e7d","executionInfo":{"status":"ok","timestamp":1583868537785,"user_tz":-120,"elapsed":73830,"user":{"displayName":"Alexey Nikolaenko","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjbbz7Ytlsc0aH3DDhRWIW8DjAuczXobhImJ_biFA=s64","userId":"05302668007181749040"}},"colab":{"base_uri":"https://localhost:8080/","height":148}},"source":["import sys, numpy as np\n","from keras.datasets import mnist\n","\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","\n","images, labels = (x_train[0:1000].reshape(1000, 28*28) / 255, y_train[0:1000])\n","one_hot_labels = np.zeros((len(labels), 10))\n","\n","for i, l in enumerate(labels):\n","  one_hot_labels[i][l] = 1\n","labels = one_hot_labels\n","\n","test_images = x_test.reshape(len(x_test), 28*28) / 255\n","test_labels = np.zeros((len(y_test), 10))\n","\n","for i, l in enumerate(y_test):\n","  test_labels[i][l] = 1\n","\n","np.random.seed(1)\n","relu = lambda x: (x > 0) * x\n","relu2deriv = lambda x: x > 0\n","alpha, iter, hidden_size, pixels_per_image, num_labels = (0.005, 350, 40, 784, 10)\n","\n","weights_0_1 = 0.2 * np.random.random((pixels_per_image, hidden_size)) - 0.1  \n","weights_1_2 = 0.2 * np.random.random((hidden_size, num_labels)) - 0.1\n","\n","for j in range(iter):\n","  error, correct_cnt = (0.0, 0)\n","\n","  for i in range(len(images)):\n","    layer_0 = images[i:i+1]\n","    layer_1 = relu(np.dot(layer_0, weights_0_1))\n","    layer_2 = np.dot(layer_1, weights_1_2)\n","    error += np.sum((layer_2 - labels[i:i+1]) ** 2)\n","    correct_cnt += int(np.argmax(layer_2) == np.argmax(labels[i:i+1]))\n","\n","    layer_2_delta = (layer_2 - labels[i:i+1])\n","    layer_1_delta = layer_2_delta.dot(weights_1_2.T) * relu2deriv(layer_1)\n","\n","    weights_1_2 -= alpha * layer_1.T.dot(layer_2_delta)\n","    weights_0_1 -= alpha * layer_0.T.dot(layer_1_delta)\n","  \n","  sys.stdout.write(\"\\r\" + \n","                   \" I:\" + str(j) + \n","                   \" Error: \" + str(error / float(len(images)))[0:5] + \n","                   \" Correct: \" + str(correct_cnt / float(len(images))))"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n","11493376/11490434 [==============================] - 0s 0us/step\n"," I:349 Error: 0.003 Correct: 0.999"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZDvuGzrn6T3b","colab_type":"text"},"source":["## **Проверка на точность работы трехслойной сети**"]},{"cell_type":"code","metadata":{"id":"_06gGPak6m-t","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"c887e7cf-c5d9-427b-e38e-8a5828abcd9f","executionInfo":{"status":"ok","timestamp":1583868797881,"user_tz":-120,"elapsed":1084,"user":{"displayName":"Alexey Nikolaenko","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjbbz7Ytlsc0aH3DDhRWIW8DjAuczXobhImJ_biFA=s64","userId":"05302668007181749040"}}},"source":["if (j % 10 == 0 or j == iter-1):\n","  error, correct_cnt = (0.0, 0)\n","  for i in range(len(test_images)):\n","    layer_0 = test_images[i:i+1]\n","    layer_1 = relu(np.dot(layer_0, weights_0_1))\n","    layer_2 = np.dot(layer_1, weights_1_2)\n","\n","    error += np.sum((layer_2 - test_labels[i:i+1]) ** 2)\n","    correct_cnt += int(np.argmax(layer_2) == np.argmax(test_labels[i:i+1]))\n","    \n","  sys.stdout.write(\" Test-Err: \" + str(error / float(len(test_images)))[0:5] + \n","                   \" Test-Acc: \" + str(correct_cnt / float(len(test_images))))\n","  print()"],"execution_count":9,"outputs":[{"output_type":"stream","text":[" Test-Err: 0.355 Test-Acc: 0.829\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"YXeRew2b9ghw","colab_type":"text"},"source":["## **Стандартный способ регуляризации: прореживание (дропаут)**"]},{"cell_type":"code","metadata":{"id":"njFcWVkk9k2o","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"82913b94-d728-4d0b-c6f7-ba2b1b334bf2","executionInfo":{"status":"ok","timestamp":1583871195250,"user_tz":-120,"elapsed":106378,"user":{"displayName":"Alexey Nikolaenko","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjbbz7Ytlsc0aH3DDhRWIW8DjAuczXobhImJ_biFA=s64","userId":"05302668007181749040"}}},"source":["import sys, numpy as np\n","from keras.datasets import mnist\n","\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","\n","images, labels = (x_train[0:1000].reshape(1000, 28*28) / 255, y_train[0:1000])\n","one_hot_labels = np.zeros((len(labels), 10))\n","\n","for i, l in enumerate(labels):\n","  one_hot_labels[i][l] = 1\n","labels = one_hot_labels\n","\n","test_images = x_test.reshape(len(x_test), 28*28) / 255\n","test_labels = np.zeros((len(y_test), 10))\n","\n","for i, l in enumerate(y_test):\n","  test_labels[i][l] = 1\n","\n","np.random.seed(1)\n","relu = lambda x: (x > 0) * x\n","relu2deriv = lambda x: x > 0\n","alpha, iter, hidden_size, pixels_per_image, num_labels = (0.005, 350, 40, 784, 10)\n","\n","weights_0_1 = 0.2 * np.random.random((pixels_per_image, hidden_size)) - 0.1  \n","weights_1_2 = 0.2 * np.random.random((hidden_size, num_labels)) - 0.1\n","\n","for j in range(iter):\n","  error, correct_cnt = (0.0, 0)\n","\n","  for i in range(len(images)):\n","    layer_0 = images[i:i+1]\n","    layer_1 = relu(np.dot(layer_0, weights_0_1))\n","    dropout_mask = np.random.randint(2, size=layer_1.shape) #dropout \"прореживание\" отключение 50% случайно выбранных узлов\n","    \n","    layer_1 *= dropout_mask * 2 #регуляризация \"прореживание\" первого слоя\n","\n","    layer_2 = np.dot(layer_1, weights_1_2)\n","    error += np.sum((layer_2 - labels[i:i+1]) ** 2)\n","    correct_cnt += int(np.argmax(layer_2) == np.argmax(labels[i:i+1]))\n","\n","    layer_2_delta = (layer_2 - labels[i:i+1])\n","    layer_1_delta = layer_2_delta.dot(weights_1_2.T) * relu2deriv(layer_1)\n","\n","    layer_1_delta *= dropout_mask #регуляризация \"прореживание\" дельты первого слоя\n","\n","    weights_1_2 -= alpha * layer_1.T.dot(layer_2_delta)\n","    weights_0_1 -= alpha * layer_0.T.dot(layer_1_delta)\n","\n","##############################################################\n","# Проверка на точность работы трехслойной сети после DropOut #\n","##############################################################\n","\n","  if (j % 10 == 0):\n","    test_error = 0.0\n","    test_correct_cnt = 0\n","\n","    for i in range(len(test_images)):\n","      layer_0 = test_images[i:i+1]\n","      layer_1 = relu(np.dot(layer_0, weights_0_1))\n","      layer_2 = np.dot(layer_1, weights_1_2)\n","\n","      test_error += np.sum((layer_2 - test_labels[i:i+1]) ** 2)\n","      test_correct_cnt += int(np.argmax(layer_2) == np.argmax(test_labels[i:i+1]))\n","\n","    sys.stdout.write(\"\\r\" + \n","                    \" I:\" + str(j) +\n","                    \" Test-Err:\" + str(test_error / float(len(test_images)))[0:5] + \n","                    \" Test-Acc:\" + str(test_correct_cnt / float(len(test_images))) +  \n","                    \" Train-Err:\" + str(error / float(len(images)))[0:5] + \n","                    \" Train-Acc:\" + str(correct_cnt / float(len(images))))"],"execution_count":14,"outputs":[{"output_type":"stream","text":[" I:340 Test-Err:0.346 Test-Acc:0.8438 Train-Err:0.271 Train-Acc:0.882"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"EQAj_FU0ADR3","colab_type":"text"},"source":["## **Пакетный градиентный спуск**"]},{"cell_type":"code","metadata":{"id":"TlSXBjDmAN8f","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"5bf74bd1-3a0b-4bf5-8927-42f5c2881386","executionInfo":{"status":"ok","timestamp":1583874442014,"user_tz":-120,"elapsed":377405,"user":{"displayName":"Alexey Nikolaenko","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjbbz7Ytlsc0aH3DDhRWIW8DjAuczXobhImJ_biFA=s64","userId":"05302668007181749040"}}},"source":["#Этот метод увеличивает скорость обучения и улучшает сходимость\n","\n","import numpy as np\n","np.random.seed(1)\n","\n","relu = lambda x: (x > 0) * x\n","relu2deriv = lambda x: x > 0 \n","\n","batch_size = 100\n","alpha, iter = (0.001, 300)\n","pixels_per_image, num_labels, hidden_size = (784, 10, 100)\n","\n","weights_0_1 = 0.2 * np.random.random((pixels_per_image, hidden_size)) - 0.1\n","weights_1_2 = 0.2 * np.random.random((hidden_size, num_labels)) - 0.1\n","\n","for j in range(iter):\n","  error, correct_cnt = (0.0, 0)\n","  for i in range(int(len(images) / batch_size)):\n","    batch_start, batch_end = ((i * batch_size), ((i+1) * batch_size))\n","\n","    layer_0 = images[batch_start:batch_end]\n","    layer_1 = relu(np.dot(layer_0, weights_0_1))\n","    dropout_mask = np.random.randint(2, size=layer_1.shape)\n","    layer_1 *= dropout_mask * 2\n","    layer_2 = np.dot(layer_1, weights_1_2)\n","\n","    error += np.sum((layer_2 - labels[batch_start:batch_end]) ** 2)\n","\n","    for k in range(batch_size):\n","      correct_cnt += int(np.argmax(layer_2[k:k+1] == np.argmax(labels[batch_start+k:batch_start+k+1])))\n","      layer_2_delta = (layer_2 - labels[batch_start:batch_end]) / batch_size\n","      layer_1_delta = layer_2_delta.dot(weights_1_2.T) * relu2deriv(layer_1)\n","      layer_1_delta *= dropout_mask\n","\n","      weights_1_2 -= alpha * layer_1.T.dot(layer_2_delta)\n","      weights_0_1 -= alpha * layer_0.T.dot(layer_1_delta)\n","\n","  if (j % 10 == 0):\n","    test_error = 0.0\n","    test_correct_cnt = 0\n","\n","    for i in range(len(test_images)):\n","      layer_0 = test_images[i:i+1]\n","      layer_1 = relu(np.dot(layer_0, weights_0_1))\n","      layer_2 = np.dot(layer_1, weights_1_2)\n","\n","    sys.stdout.write(\"\\r\" + \n","                    \" I:\" + str(j) +\n","                    \" Test-Err:\" + str(test_error / float(len(test_images)))[0:5] + \n","                    \" Test-Acc:\" + str(test_correct_cnt / float(len(test_images))) +  \n","                    \" Train-Err:\" + str(error / float(len(images)))[0:5] + \n","                    \" Train-Acc:\" + str(correct_cnt / float(len(images))))"],"execution_count":20,"outputs":[{"output_type":"stream","text":[" I:290 Test-Err:0.0 Test-Acc:0.0 Train-Err:0.224 Train-Acc:0.0"],"name":"stdout"}]}]}